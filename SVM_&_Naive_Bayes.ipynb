{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive Bayes | Assignment"
      ],
      "metadata": {
        "id": "b08KXfyHszny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "**Definition:**  \n",
        "Support Vector Machine (SVM) is a **supervised machine learning algorithm** mainly used for classification and regression problems. It works by finding the **optimal hyperplane** that separates data points of different classes with the **maximum margin**. The points that lie closest to the hyperplane and influence its position are called **support vectors**.\n",
        "\n",
        "**Working Principle:**  \n",
        "- SVM transforms data into a **higher-dimensional space** if required (using kernel functions) to make it linearly separable.  \n",
        "- It identifies the **hyperplane** that **maximizes the margin** between classes.  \n",
        "- **Support vectors** are the key data points that define the hyperplane.  \n",
        "- **Kernel trick** allows SVM to handle non-linear data effectively (Linear, Polynomial, RBF kernels).\n",
        "\n",
        "**Example/Use-case:**  \n",
        "- Email spam detection (spam vs non-spam)  \n",
        "- Image classification  \n",
        "- Handwriting recognition\n",
        "\n",
        "# Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "**Hard Margin SVM:**  \n",
        "- Requires **perfectly linearly separable data**.  \n",
        "- No misclassification allowed; tries to **maximize the margin** strictly.  \n",
        "- Sensitive to **outliers**; even a single outlier can affect the hyperplane.\n",
        "\n",
        "**Soft Margin SVM:**  \n",
        "- Allows **some misclassification** for better generalization.  \n",
        "- Introduces a **penalty parameter (C)** to balance margin size and misclassification.  \n",
        "- Works well with **noisy or overlapping data**.\n",
        "\n",
        "**Example:**  \n",
        "- Hard Margin → Clean dataset with well-separated classes  \n",
        "- Soft Margin → Real-world dataset with overlapping classes\n",
        "\n",
        "# Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "**Kernel Trick:**  \n",
        "- A technique in SVM to handle **non-linear data** by transforming it into a **higher-dimensional space** where it becomes linearly separable.  \n",
        "- It allows SVM to find an optimal hyperplane **without explicitly computing the transformation**.  \n",
        "\n",
        "**Example – RBF (Radial Basis Function) Kernel:**  \n",
        "- Measures similarity between points using a **Gaussian function**.\n",
        "\n",
        "**Use Case:**  \n",
        "- Works well in **image classification** where data is complex and non-linearly separable.  \n",
        "\n",
        "**Summary:**  \n",
        "- Kernel trick → handles non-linear data efficiently  \n",
        "- RBF kernel → popular for complex datasets\n",
        "\n",
        "# Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        " **Definition:-**\n",
        "- Naïve Bayes is a **supervised machine learning algorithm** used for classification.  \n",
        "- It is based on **Bayes’ Theorem**, which calculates the probability of a class given the input features.\n",
        "\n",
        "**Why “Naïve”?:-**\n",
        "- Called **naïve** because it assumes that **all features are independent** of each other,  \n",
        "  which is rarely true in real-world data, but simplifies computation.\n",
        "\n",
        "**Example:-**\n",
        "- Email spam detection (predicting spam vs non-spam)\n",
        "\n",
        "# Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "* **Gaussian Naïve Bayes:**  \n",
        "  - Assumes that **features follow a normal (Gaussian) distribution**.  \n",
        "  - **Use case:** Continuous data, e.g., **Iris dataset** (flower measurements).\n",
        "\n",
        "* **Multinomial Naïve Bayes:**  \n",
        "  - Works with **discrete count data**, like word frequencies.  \n",
        "  - **Use case:** Text classification, e.g., **spam detection** or **news categorization**.\n",
        "\n",
        "* **Bernoulli Naïve Bayes:**  \n",
        "  - Works with **binary/Boolean features** (0 or 1).  \n",
        "  - **Use case:** Presence/absence of a feature, e.g., **email word occurrence** in spam detection.\n",
        "\n",
        "* **Dataset Info:**  \n",
        "  -  You Can use any datasets from `sklearn.datasets` like **Iris**, **Breast Cancer**, **Wine**, or your own CSV file.\n",
        "\n",
        "\n",
        "  # PRACTICAL QUESTION WITH ANSWER:-\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "GQ9kAeDkxyGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "k5lYPVvoBE1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu7iO6x4smzg",
        "outputId": "93e88fb1-6436-4aff-93d8-4429ef46c8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# Question 6: SVM on Iris dataset\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print support vectors\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "oNn9mp13Bf5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Gaussian Naïve Bayes on Breast Cancer dataset\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes model\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnx0R6BnBlQP",
        "outputId": "4a87c89f-5ae4-408f-81e4-dacad4806ade"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "lUccFO1aBzt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: SVM on Wine dataset with GridSearchCV\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define SVM and parameter grid\n",
        "svm = SVC()\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# GridSearchCV to find best hyperparameters\n",
        "grid = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Print best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84gLBviqB-gR",
        "outputId": "d5338886-f66d-4df9-e20a-acc1fb66c580"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "oXf3dahRCD9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Naïve Bayes on 20 Newsgroups text dataset with ROC-AUC\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset (subset for faster execution)\n",
        "categories = ['rec.autos', 'sci.space', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Binarize labels for ROC-AUC\n",
        "y_bin = label_binarize(y, classes=np.unique(y))\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Multinomial Naïve Bayes with OneVsRest for multi-class ROC-AUC\n",
        "nb_model = OneVsRestClassifier(MultinomialNB())\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Compute ROC-AUC score (macro average)\n",
        "roc_score = roc_auc_score(y_test, y_prob, average='macro')\n",
        "print(\"ROC-AUC Score:\", roc_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCw5IVCjCJiX",
        "outputId": "26364de2-6cfe-49b7-ea88-cd5f27a22d9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9940406924865924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "A9XWkDqjDpaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Email Spam Classification\n",
        "\n",
        "## 1. Data Preprocessing\n",
        "* Handle missing data by **filling or dropping nulls**.  \n",
        "* Convert text into numerical features using **TF-IDF vectorization**.  \n",
        "* Optionally, remove stopwords, punctuation, and perform **lowercasing/stemming**.\n",
        "\n",
        "## 2. Model Selection\n",
        "* **Naïve Bayes** is suitable for **text classification**, especially with discrete features (word counts or TF-IDF).  \n",
        "* **SVM** can also be used, especially with **high-dimensional sparse text data**.  \n",
        "* Given the **text nature** and **simplicity**, Naïve Bayes is preferred for speed and interpretability.\n",
        "\n",
        "## 3. Handling Class Imbalance\n",
        "* Use **class weights** (for SVM) or **oversampling/undersampling**.  \n",
        "* In Naïve Bayes, imbalance can be addressed by **adjusting prior probabilities**.\n",
        "\n",
        "## 4. Performance Evaluation\n",
        "* Metrics: **Accuracy, Precision, Recall, F1-Score, ROC-AUC**.  \n",
        "* For imbalanced data, **Precision and Recall** are more important than Accuracy.\n",
        "\n",
        "## 5. Business Impact\n",
        "* Automatic spam detection improves **email productivity**, reduces **security risks**, and enhances **user satisfaction**.  \n",
        "* Helps company **filter malicious emails** before they reach users.\n"
      ],
      "metadata": {
        "id": "jhJQwbphD1bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Email Spam Classification with Naïve Bayes\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load dataset (SMS Spam dataset from UCI via URL for demo)\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', header=None, names=['class','text'])\n",
        "\n",
        "# Handle missing data\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Features and target\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Convert target to binary (ham=0, spam=1)\n",
        "lb = LabelBinarizer()\n",
        "y_bin = lb.fit_transform(y)\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_vect = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split dataset (stratify to maintain class balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vect, y_bin, test_size=0.3, random_state=42, stratify=y_bin)\n",
        "\n",
        "# Train Naïve Bayes (Multinomial)\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = nb_model.predict(X_test)\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "roc_score = roc_auc_score(y_test, y_prob[:,1])\n",
        "print(\"ROC-AUC Score:\", roc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_W7elGgE4lR",
        "outputId": "2d293934-5a4d-4070-ef94-e919e12a76cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      1448\n",
            "           1       1.00      0.75      0.86       224\n",
            "\n",
            "    accuracy                           0.97      1672\n",
            "   macro avg       0.98      0.88      0.92      1672\n",
            "weighted avg       0.97      0.97      0.97      1672\n",
            "\n",
            "ROC-AUC Score: 0.9877170481452249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    }
  ]
}